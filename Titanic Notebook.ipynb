{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add All the Models Libraries\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Common data processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from scipy import sparse\n",
    "\n",
    "#Accuracy Score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the data for feature engineering and later split it, just before applying Data Pipeline\n",
    "train_df = pd.read_csv(\"U:\\\\Titanic Dataset\\\\train.csv\") #read the data from the csv file.\n",
    "test_df = pd.read_csv(\"U:\\\\Titanic Dataset\\\\test.csv\")\n",
    "passenger_id_test = test_df[\"PassengerId\"].copy()\n",
    "DataFile = TrainFile.append(TestFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.881138</td>\n",
       "      <td>33.295479</td>\n",
       "      <td>0.385027</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.413493</td>\n",
       "      <td>51.758668</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>378.020061</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>1.041658</td>\n",
       "      <td>0.486592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>982.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age         Fare        Parch  PassengerId       Pclass  \\\n",
       "count  1046.000000  1308.000000  1309.000000  1309.000000  1309.000000   \n",
       "mean     29.881138    33.295479     0.385027   655.000000     2.294882   \n",
       "std      14.413493    51.758668     0.865560   378.020061     0.837836   \n",
       "min       0.170000     0.000000     0.000000     1.000000     1.000000   \n",
       "25%      21.000000     7.895800     0.000000   328.000000     2.000000   \n",
       "50%      28.000000    14.454200     0.000000   655.000000     3.000000   \n",
       "75%      39.000000    31.275000     0.000000   982.000000     3.000000   \n",
       "max      80.000000   512.329200     9.000000  1309.000000     3.000000   \n",
       "\n",
       "             SibSp    Survived  \n",
       "count  1309.000000  891.000000  \n",
       "mean      0.498854    0.383838  \n",
       "std       1.041658    0.486592  \n",
       "min       0.000000    0.000000  \n",
       "25%       0.000000    0.000000  \n",
       "50%       0.000000    0.000000  \n",
       "75%       1.000000    1.000000  \n",
       "max       8.000000    1.000000  "
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFile.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      "Age            1046 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Name           1309 non-null object\n",
      "Parch          1309 non-null int64\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Sex            1309 non-null object\n",
      "SibSp          1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Ticket         1309 non-null object\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 132.9+ KB\n"
     ]
    }
   ],
   "source": [
    "DataFile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Split the names to gt Mr. or Miss or Mrs.\n",
    "\n",
    "FirstName = DataFile[\"Name\"].str.split(\"[,.]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now strip the white spaces from the Salutation\n",
    "titles = [str.strip(name[1]) for name in FirstName.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFile[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the columns - that may not impact the analysis\n",
    "DataFile = DataFile.drop('Name',axis=1)\n",
    "DataFile = DataFile.drop('PassengerId',axis=1)\n",
    "DataFile = DataFile.drop('Embarked',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split the Tickets to get special cabins. As Tickets may hold some valuable insights\n",
    "\n",
    "DataFile['Ticket'] = DataFile['Ticket'].apply(lambda x: str(x)[0])\n",
    "\n",
    "#Replace All the number values with N\n",
    "\n",
    "DataFile['Ticket'].replace(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], 'N', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now take the first letter of the Cabin and Impute O for other cabins imputed\n",
    "\n",
    "DataFile[[\"Cabin\"]] = DataFile[[\"Cabin\"]].fillna(value=\"O\")\n",
    "DataFile[\"Cabin\"] = DataFile['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now first we replace the extra titles to Mr and Mrs\n",
    "\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "\n",
    "DataFile.replace({'Title': mapping}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the imputed value for FARE\n",
    "DataFile['Fare'].fillna(DataFile['Fare'].median(), inplace=True)\n",
    "\n",
    "#impute the age based on Titles \n",
    "titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n",
    "for title in titles:\n",
    "    imputed_age = DataFile.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    DataFile.loc[(DataFile['Age'].isnull()) & (DataFile['Title'] == title), 'Age'] = imputed_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SibSp and Parch into one\n",
    "DataFile[\"Family Size\"] = DataFile[\"SibSp\"] + DataFile[\"Parch\"]\n",
    "\n",
    "#drop SibSp and Parch\n",
    "DataFile = DataFile.drop('SibSp',axis=1)\n",
    "DataFile = DataFile.drop('Parch',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Fare bins\n",
    "\n",
    "DataFile['FareBin'] = pd.qcut(DataFile['Fare'], 5)\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataFile['FareBin'] = label.fit_transform(DataFile['FareBin'])\n",
    "DataFile = DataFile.drop('Fare',axis=1)\n",
    "\n",
    "#Making Age Bins\n",
    "DataFile['AgeBin'] = pd.qcut(DataFile['Age'], 4)\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataFile['AgeBin'] = label.fit_transform(DataFile['AgeBin'])\n",
    "DataFile = DataFile.drop('Age',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dummy for male and female\n",
    "DataFile['Sex'].replace(['male','female'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the file \n",
    "\n",
    "#DataFile.to_csv(\"Datafile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The data to training and test set - before applying the pipeline\n",
    "\n",
    "train_set, test_set = train_test_split(DataFile, test_size=0.3193,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 9)"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape # This exactly matches the original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 9)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape # This exactly matches the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Observations</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgeBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FareBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family Size</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Number of Observations  Percent\n",
       "AgeBin                            0      0.0\n",
       "FareBin                           0      0.0\n",
       "Family Size                       0      0.0\n",
       "Title                             0      0.0\n",
       "Ticket                            0      0.0\n",
       "Survived                          0      0.0\n",
       "Sex                               0      0.0\n",
       "Pclass                            0      0.0\n",
       "Cabin                             0      0.0"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n",
    "\n",
    "obs = train_set.isnull().sum().sort_values(ascending = False)\n",
    "percent = round(train_set.isnull().sum().sort_values(ascending = False)/len(train_set)*100, 2)\n",
    "pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Observations</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>418</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgeBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FareBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family Size</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Number of Observations  Percent\n",
       "Survived                        418    100.0\n",
       "AgeBin                            0      0.0\n",
       "FareBin                           0      0.0\n",
       "Family Size                       0      0.0\n",
       "Title                             0      0.0\n",
       "Ticket                            0      0.0\n",
       "Sex                               0      0.0\n",
       "Pclass                            0      0.0\n",
       "Cabin                             0      0.0"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n",
    "\n",
    "obs = test_set.isnull().sum().sort_values(ascending = False)\n",
    "percent = round(test_set.isnull().sum().sort_values(ascending = False)/len(test_set)*100, 2)\n",
    "pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now define x and y.\n",
    "\n",
    "#the Y Variable\n",
    "train_set_y = train_set[\"Survived\"].copy()\n",
    "test_set_y = test_set[\"Survived\"].copy()\n",
    "\n",
    "#the X variables\n",
    "train_set_X = train_set.drop(\"Survived\", axis=1)\n",
    "test_set_X = test_set.drop(\"Survived\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here Starts the Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector([\"Ticket\", \"Title\",\"Cabin\"])),\n",
    "        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n",
    "    ])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector([\"Pclass\",\"Family Size\",\"FareBin\", \"AgeBin\"])),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "      ])\n",
    "\n",
    "no_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector([\"Sex\"]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"no_pipeline\", no_pipeline),\n",
    "    ])\n",
    "\n",
    "final_train_X = full_pipeline.fit_transform(train_set_X)\n",
    "final_test_X = full_pipeline.transform(test_set_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now We Build the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 220 candidates, totalling 880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 476 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 880 out of 880 | elapsed:   14.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24], 'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1044,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Introduce KNN Classifier \n",
    "\n",
    "KNeighbours = KNeighborsClassifier()\n",
    "leaf_size = list(range(1,50,5))\n",
    "n_neighbors = list(range(4,25,2))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors,\n",
    "'algorithm' : ['auto'],\n",
    "'weights' : ['uniform', 'distance'],\n",
    "'leaf_size':leaf_size }\n",
    "\n",
    "grid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84175084175084181"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_grid = grid_search_KNeighbours.best_estimator_\n",
    "\n",
    "y_pred_neighbor_grid = neighbor_grid.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_neighbor_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another KNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 320 candidates, totalling 1280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 722 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1280 out of 1280 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [15, 16, 17, 18, 19], 'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighbours2 = KNeighborsClassifier()\n",
    "leaf_size2 = list(range(18,50,1))\n",
    "n_neighbors2 = list(range(15,20,1))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors2,\n",
    "'algorithm' : ['auto'],\n",
    "'weights' : ['uniform', 'distance'],\n",
    "'leaf_size':leaf_size2}\n",
    "\n",
    "grid_search_KNeighbours2 = GridSearchCV(KNeighbours2, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours2.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84399551066217737"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_grid2 = grid_search_KNeighbours2.best_estimator_\n",
    "\n",
    "y_pred_neighbor_grid2 = neighbor_grid2.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_neighbor_grid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  40 | elapsed:    5.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 50], 'max_features': [0.1, 0.5], 'max_depth': [2, 10, 20], 'min_samples_split': [0.1, 0.5], 'oob_score': [True, False], 'min_samples_leaf': [0.1, 0.5], 'max_leaf_nodes': [2, 10, 50]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [10, 50]\n",
    "max_features = [0.1, 0.5]\n",
    "max_depth = [2, 10, 20] \n",
    "oob_score = [True, False]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5] \n",
    "max_leaf_nodes = [2, 10, 50]\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n",
    "                     'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78675645342312006"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_estimator = rand_search_forest.best_estimator_\n",
    "\n",
    "y_pred_random_estimator = random_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_random_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 20, 50, 70, 90], 'learning_rate': [0.1, 0.5, 0.9], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 20, 50, 70, 90]\n",
    "learning_rate = [0.1, 0.5, 0.9]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83052749719416386"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_estimator = rand_search_ada.best_estimator_\n",
    "\n",
    "y_pred_ada_estimator = ada_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_ada_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  40 | elapsed:    5.8s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 40, 60, 80], 'max_features': [0.1, 0.5], 'max_depth': [2, 50, 100], 'min_samples_split': [0.1, 0.5], 'min_samples_leaf': [0.1, 0.5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_classifier = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 40, 60, 80]\n",
    "max_features = [0.1, 0.5]\n",
    "max_depth = [2, 50, 100]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5] # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79461279461279466"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_estimator = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "y_pred_extra_estimator = extra_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_extra_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=86004.5806744, gamma=0.0128162967611 ..........................\n",
      "[CV] ........... C=86004.5806744, gamma=0.0128162967611, total=   1.3s\n",
      "[CV] C=86004.5806744, gamma=0.0128162967611 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... C=86004.5806744, gamma=0.0128162967611, total=   1.6s\n",
      "[CV] C=86004.5806744, gamma=0.0128162967611 ..........................\n",
      "[CV] ........... C=86004.5806744, gamma=0.0128162967611, total=   3.4s\n",
      "[CV] C=97577.1210544, gamma=0.0121024495154 ..........................\n",
      "[CV] ........... C=97577.1210544, gamma=0.0121024495154, total=   1.3s\n",
      "[CV] C=97577.1210544, gamma=0.0121024495154 ..........................\n",
      "[CV] ........... C=97577.1210544, gamma=0.0121024495154, total=   2.0s\n",
      "[CV] C=97577.1210544, gamma=0.0121024495154 ..........................\n",
      "[CV] ........... C=97577.1210544, gamma=0.0121024495154, total=   3.0s\n",
      "[CV] C=13503.3168534, gamma=0.000375321873496 ........................\n",
      "[CV] ......... C=13503.3168534, gamma=0.000375321873496, total=   0.0s\n",
      "[CV] C=13503.3168534, gamma=0.000375321873496 ........................\n",
      "[CV] ......... C=13503.3168534, gamma=0.000375321873496, total=   0.0s\n",
      "[CV] C=13503.3168534, gamma=0.000375321873496 ........................\n",
      "[CV] ......... C=13503.3168534, gamma=0.000375321873496, total=   0.0s\n",
      "[CV] C=89560.4588641, gamma=0.00928761481096 .........................\n",
      "[CV] .......... C=89560.4588641, gamma=0.00928761481096, total=   1.1s\n",
      "[CV] C=89560.4588641, gamma=0.00928761481096 .........................\n",
      "[CV] .......... C=89560.4588641, gamma=0.00928761481096, total=   1.1s\n",
      "[CV] C=89560.4588641, gamma=0.00928761481096 .........................\n",
      "[CV] .......... C=89560.4588641, gamma=0.00928761481096, total=   1.8s\n",
      "[CV] C=54187.9271466, gamma=0.00187818827703 .........................\n",
      "[CV] .......... C=54187.9271466, gamma=0.00187818827703, total=   0.4s\n",
      "[CV] C=54187.9271466, gamma=0.00187818827703 .........................\n",
      "[CV] .......... C=54187.9271466, gamma=0.00187818827703, total=   0.3s\n",
      "[CV] C=54187.9271466, gamma=0.00187818827703 .........................\n",
      "[CV] .......... C=54187.9271466, gamma=0.00187818827703, total=   0.4s\n",
      "[CV] C=38454.9198267, gamma=0.73037390794 ............................\n",
      "[CV] ............. C=38454.9198267, gamma=0.73037390794, total=   0.0s\n",
      "[CV] C=38454.9198267, gamma=0.73037390794 ............................\n",
      "[CV] ............. C=38454.9198267, gamma=0.73037390794, total=   0.0s\n",
      "[CV] C=38454.9198267, gamma=0.73037390794 ............................\n",
      "[CV] ............. C=38454.9198267, gamma=0.73037390794, total=   0.0s\n",
      "[CV] C=53296.9330783, gamma=0.343567544284 ...........................\n",
      "[CV] ............ C=53296.9330783, gamma=0.343567544284, total=   0.0s\n",
      "[CV] C=53296.9330783, gamma=0.343567544284 ...........................\n",
      "[CV] ............ C=53296.9330783, gamma=0.343567544284, total=   0.0s\n",
      "[CV] C=53296.9330783, gamma=0.343567544284 ...........................\n",
      "[CV] ............ C=53296.9330783, gamma=0.343567544284, total=   0.0s\n",
      "[CV] C=74816.3123206, gamma=0.27146296301 ............................\n",
      "[CV] ............. C=74816.3123206, gamma=0.27146296301, total=   0.0s\n",
      "[CV] C=74816.3123206, gamma=0.27146296301 ............................\n",
      "[CV] ............. C=74816.3123206, gamma=0.27146296301, total=   0.0s\n",
      "[CV] C=74816.3123206, gamma=0.27146296301 ............................\n",
      "[CV] ............. C=74816.3123206, gamma=0.27146296301, total=   0.0s\n",
      "[CV] C=95244.9544774, gamma=0.668725849569 ...........................\n",
      "[CV] ............ C=95244.9544774, gamma=0.668725849569, total=   0.0s\n",
      "[CV] C=95244.9544774, gamma=0.668725849569 ...........................\n",
      "[CV] ............ C=95244.9544774, gamma=0.668725849569, total=   0.0s\n",
      "[CV] C=95244.9544774, gamma=0.668725849569 ...........................\n",
      "[CV] ............ C=95244.9544774, gamma=0.668725849569, total=   0.0s\n",
      "[CV] C=79794.2236203, gamma=0.166566531223 ...........................\n",
      "[CV] ............ C=79794.2236203, gamma=0.166566531223, total=   0.0s\n",
      "[CV] C=79794.2236203, gamma=0.166566531223 ...........................\n",
      "[CV] ............ C=79794.2236203, gamma=0.166566531223, total=   0.0s\n",
      "[CV] C=79794.2236203, gamma=0.166566531223 ...........................\n",
      "[CV] ............ C=79794.2236203, gamma=0.166566531223, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000000012F4BCC0>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000000012FD5B00>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_Classifier = SVC(random_state = 42)\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.0001, 1), \"C\": uniform(10000, 100000)}\n",
    "\n",
    "rand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = 1)\n",
    "\n",
    "rand_search_svc.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8630751964085297"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_estimator = rand_search_svc.best_estimator_\n",
    "\n",
    "y_pred_svc_estimator = svc_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_svc_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 100], 'learning_rate': [0.1, 0.5], 'max_depth': [3, 50, 70], 'min_samples_split': [0.1, 0.5], 'min_samples_leaf': [0.1, 0.5], 'max_features': [0.1, 0.5], 'max_leaf_nodes': [2, 50, 70]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_Classifier = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 100]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [3, 50, 70]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5]\n",
    "max_features = [0.1, 0.5]\n",
    "max_leaf_nodes = [2, 50, 70]\n",
    "                            \n",
    "param_grid_grad_boost_class = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n",
    "                              'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "rand_search_grad_boost_class = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost_class, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost_class.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82491582491582494"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_estimator = rand_search_grad_boost_class.best_estimator_\n",
    "\n",
    "y_pred_gb_estimator = gb_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_gb_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  40 | elapsed:    6.0s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': array([ 0.1,  0.2, ...,  9.8,  9.9])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = np.array(list(range(1, 100)))/10\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83389450056116721"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_estimator = rand_search_log_reg.best_estimator_\n",
    "\n",
    "y_pred_log_estimator = log_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_log_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC looks like a clear winner with 86% accuracy on training set. KNN2 is also a candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_svc_rand = svc_estimator.predict(final_test_X)\n",
    "\n",
    "#predict using k neighbors 3\n",
    "y_pred_knn_grid = neighbor_grid.predict(final_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the datafile for SVC\n",
    "result_test1 = pd.DataFrame()\n",
    "passenger_id_test = test_df[\"PassengerId\"].copy()\n",
    "result_test1[\"PassengerId\"] = passenger_id_test\n",
    "result_test1[\"Survived\"] = y_pred_svc_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the datafile for KNN 3\n",
    "result_test2 = pd.DataFrame()\n",
    "passenger_id_test = test_df[\"PassengerId\"].copy()\n",
    "result_test2[\"PassengerId\"] = passenger_id_test\n",
    "result_test2[\"Survived\"] = y_pred_knn_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test1.to_csv(\"Titanic_prediction_ashish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test2.to_csv(\"Titanic_prediction_ashish2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
